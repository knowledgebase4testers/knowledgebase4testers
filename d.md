---
layout: default
title:  QA Glossary - Letter D
description: This is just Letter D
---

### Daily Scrum 
- also referred to as the daily stand-up, is a (daily) recurring time-boxed meeting (e.g. 15 minutes or less) at which team members are taking turns answering three questions: 
> 1. What did I accomplish since the last daily Scrum? 
> 2. What do I plan to work on by the next daily Scrum? 
> 3. What are the obstacles or impediments that are preventing me from making progress?

### data definition: 
- An executable statement where a variable is assigned a value.

### data definition C-use coverage: 
- The percentage of data definition C-use pairs in a
component that are exercised by a test case suite.

### data definition C-use pair: 
- A data definition and computation data use, where the data
use uses the value defined in the data definition.

### data definition P-use coverage: 
- The percentage of data definition P-use pairs in a
component that are exercised by a test case suite.

### data definition P-use pair: 
- A data definition and predicate data use, where the data use
uses the value defined in the data definition.

### data definition-use coverage: 
- The percentage of data definition-use pairs in a component
that are exercised by a test case suite.

### data definition-use pair: 
- A data definition and data use, where the data use uses the
value defined in the data definition.

### data definition-use testing: 
- A test case design technique for a component in which test
cases are designed to execute data definition-use pairs.

### data flow coverage: 
- Test coverage measure based on variable usage within the code.
- Examples are data definition-use coverage, data definition P-use coverage, data
definition C-use coverage, etc.

### data flow testing: 
- Testing in which test cases are designed based on variable usage
within the code.

### data use: 
- An executable statement where the value of a variable is accessed.

### debugging: 
- The process of finding bugs and removing/fixing the causes of failures in software.
1. Identify the error (the earlier the better)
2. Identify the error location
3. Analyze the error (what type of error)
4. Cover the lateral damage (bug can be resolved by making changes)
5. Fix and validate

### Decision matrix
- 

### decision coverage: 
- The percentage of decision outcomes that have been exercised by a
test case suite.

### decision outcome: 
- The result of a decision (which therefore determines the control flow
alternative taken).

### design-based testing: 
- Designing tests based on objectives derived from the architectural
or detail design of the software (e.g., tests that execute specific invocation paths or
probe the worst case behaviour of algorithms).

### desk checking: 
- The testing of software by the manual simulation of its execution.

### dirty testing: 
- See negative testing. [Beizer]

### documentation testing: 
- Testing concerned with the accuracy of documentation.

### DOD (Definition of Done) 
- assessing when a user story has been completed.
- 

### domain: 
- The set from which values are selected.

### domain testing: 
- application is tested by giving a minimum number of inputs and evaluating its appropriate outputs. 
- The primary goal of Domain testing is to check whether the software application accepts inputs within the acceptable range and delivers required output.

### dynamic analysis: 
- The process of evaluating a system or component based upon its behaviour during execution. [IEEE]

[back](./)